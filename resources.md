# Resources

## Vision transformers

- Focal self-attention for local-global interactions in vision transformers (focal
  transformer). 
  [\[paper\]](https://arxiv.org/pdf/2107.00641.pdf) 
  [\[github\]](https://github.com/microsoft/Focal-Transformer)

## Explainability

- Exploring explainability for vision transformers. 
  [\[blog\]](https://jacobgil.github.io/deeplearning/vision-transformer-explainability) 
  [\[github\]](https://github.com/jacobgil/vit-explain) 
- Transformer explainability beyond attention visualization 
  [\[paper\]](https://arxiv.org/abs/2012.09838) 
  [\[github\]](https://github.com/hila-chefer/Transformer-Explainability) 
- Generic attention-model explainability for interpreting bi-modal and encoder-decoder
  transformers. 
  [\[paper\]](https://arxiv.org/pdf/2103.15679.pdf) 
  [\[github\]](https://github.com/hila-chefer/Transformer-MM-Explainability) 

## Code

- [vit-pytorch](https://github.com/lucidrains/vit-pytorch) repository with 
  implementations of several transformer architectures. No pretrained weights, just
  architectures.
- [External-attention-PyTorch](https://github.com/xmu-xiaoma666/External-Attention-pytorch) 
  repository of many attention architectures. No pretrained weights.
  