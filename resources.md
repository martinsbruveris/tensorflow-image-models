# Resources

## ResNets

- [Big Transer](https://github.com/google-research/big_transfer) (BiT) repository by 
  Google with TF2 code and weights for 12 large ResNets.  
  [\[paper\]](https://arxiv.org/pdf/1912.11370.pdf)


## Vision transformers

- Focal self-attention for local-global interactions in vision transformers (focal
  transformer). 
  [\[paper\]](https://arxiv.org/pdf/2107.00641.pdf) 
  [\[github\]](https://github.com/microsoft/Focal-Transformer)

## Training methods

- TransMix: Attend to Mix for Vision Transformers. 
  [\[paper\]](https://arxiv.org/pdf/2111.09833.pdf)
  [\[github\]](https://github.com/beckschen/transmix)

## Explainability

- Exploring explainability for vision transformers. 
  [\[blog\]](https://jacobgil.github.io/deeplearning/vision-transformer-explainability) 
  [\[github\]](https://github.com/jacobgil/vit-explain) 
- Transformer explainability beyond attention visualization 
  [\[paper\]](https://arxiv.org/abs/2012.09838) 
  [\[github\]](https://github.com/hila-chefer/Transformer-Explainability) 
- Generic attention-model explainability for interpreting bi-modal and encoder-decoder
  transformers. 
  [\[paper\]](https://arxiv.org/pdf/2103.15679.pdf) 
  [\[github\]](https://github.com/hila-chefer/Transformer-MM-Explainability) 

## Collections

- [Transformer-in-vision](https://github.com/Yangzhangcst/Transformer-in-Computer-Vision)
  list of resources and papers on vision transformers.
- [vit-pytorch](https://github.com/lucidrains/vit-pytorch) repository with 
  implementations of several transformer architectures. No pretrained weights, just
  architectures.
- [External-attention-PyTorch](https://github.com/xmu-xiaoma666/External-Attention-pytorch) 
  repository of many attention architectures. No pretrained weights.
- How to publish a package to PyPi using GitHub Actions and poetry. 
  [\[article\]](https://dev.to/iancleary/test-and-publish-a-python-package-to-pypi-using-poetry-and-github-actions-186a)
